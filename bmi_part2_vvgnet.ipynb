{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1c233f07-f0a2-4523-b53b-bd2e36160e82",
      "metadata": {
        "id": "1c233f07-f0a2-4523-b53b-bd2e36160e82"
      },
      "source": [
        "# Face-to-BMI\n",
        "***\n",
        "## Machine Learning II\n",
        "\n",
        "By Amulya Jayanti | Halleluya Mengesha | Hira Stanley | Sami Naeem | Vaishnavi Kokadwar\n",
        "  \n",
        "*May, 2025*\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c8cb345",
      "metadata": {},
      "source": [
        "# PART 2 - Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "329b88e8",
      "metadata": {},
      "source": [
        "## c) VGGNet - Third Best Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee4896c1",
      "metadata": {},
      "source": [
        "### i) Finetuning: CNN + Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6686942c",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/halleluyamengesha/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Working Directory: /Users/halleluyamengesha/Desktop/UChicago/_Quarter_3/Machine Learning II/Project\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "import absl.logging\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "absl.logging.set_verbosity(absl.logging.ERROR)\n",
        "\n",
        "print(\"Current Working Directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fde1de4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correct path based on nested folder\n",
        "base_dir = '/Users/halleluyamengesha/Desktop/UChicago/_Quarter_3/Machine Learning II/Project/BMI/Data'\n",
        "csv_path = os.path.join(base_dir, 'data.csv')\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Create full image paths\n",
        "df['image_path'] = df['name'].apply(lambda x: os.path.join(base_dir, x))\n",
        "\n",
        "# Clean\n",
        "df.drop(columns=[\"Unnamed: 0\"], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "674b83c9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correct image folder path\n",
        "image_dir = '/Users/halleluyamengesha/Desktop/UChicago/_Quarter_3/Machine Learning II/Project/BMI/Data/Images'\n",
        "\n",
        "# Update the full image paths\n",
        "df['image_path'] = df['name'].apply(lambda x: os.path.join(image_dir, x.strip()))\n",
        "\n",
        "# Check if the images now exist\n",
        "df['image_exists'] = df['image_path'].apply(os.path.exists)\n",
        "\n",
        "# Keep only available images\n",
        "df_valid = df[df['image_exists']].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebff25f4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3210 validated image filenames.\n",
            "Found 752 validated image filenames.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-24 19:28:21.534327: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
            "2025-05-24 19:28:21.534492: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
            "2025-05-24 19:28:21.534500: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1748132901.534994  909474 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "I0000 00:00:1748132901.535056  909474 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-24 19:28:23.195117: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 684ms/step - loss: 14.7560 - val_loss: 6.8651 - learning_rate: 1.0000e-04\n",
            "Epoch 2/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 694ms/step - loss: 6.8199 - val_loss: 6.3878 - learning_rate: 1.0000e-04\n",
            "Epoch 3/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 688ms/step - loss: 5.9607 - val_loss: 5.5083 - learning_rate: 1.0000e-04\n",
            "Epoch 4/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 692ms/step - loss: 5.4618 - val_loss: 6.0912 - learning_rate: 1.0000e-04\n",
            "Epoch 5/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 606ms/step - loss: 5.3138\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 685ms/step - loss: 5.3135 - val_loss: 6.0960 - learning_rate: 1.0000e-04\n",
            "Epoch 6/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 681ms/step - loss: 5.1584 - val_loss: 6.4997 - learning_rate: 5.0000e-05\n",
            "Epoch 7/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 693ms/step - loss: 4.9089 - val_loss: 5.2836 - learning_rate: 5.0000e-05\n",
            "Epoch 8/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 677ms/step - loss: 4.5517 - val_loss: 5.2892 - learning_rate: 5.0000e-05\n",
            "Epoch 9/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601ms/step - loss: 4.4153\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 679ms/step - loss: 4.4149 - val_loss: 5.4561 - learning_rate: 5.0000e-05\n",
            "Epoch 10/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 670ms/step - loss: 4.2224 - val_loss: 5.0629 - learning_rate: 2.5000e-05\n",
            "Epoch 11/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 670ms/step - loss: 4.1677 - val_loss: 5.1724 - learning_rate: 2.5000e-05\n",
            "Epoch 12/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 601ms/step - loss: 4.0386\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 680ms/step - loss: 4.0390 - val_loss: 5.2157 - learning_rate: 2.5000e-05\n",
            "Epoch 13/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 668ms/step - loss: 3.9547 - val_loss: 5.0050 - learning_rate: 1.2500e-05\n",
            "Epoch 14/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 669ms/step - loss: 3.8485 - val_loss: 5.0306 - learning_rate: 1.2500e-05\n",
            "Epoch 15/15\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 589ms/step - loss: 3.8650\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n",
            "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 667ms/step - loss: 3.8641 - val_loss: 5.2035 - learning_rate: 1.2500e-05\n",
            "\u001b[1m24/24\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 315ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… VGG19 Fine-Tuned (Top 8 Layers + Augmentation + LR Scheduler):\n",
            "ğŸ“ˆ Pearson r (Overall): 0.634\n",
            "ğŸ‘¦ Pearson r (Male):    0.677\n",
            "ğŸ‘§ Pearson r (Female):  0.584\n",
            "ğŸ“ Mean Absolute Error: 5.00\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input  # works for VGG19 too\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from scipy.stats import pearsonr\n",
        "import numpy as np\n",
        "\n",
        "# --- Split train/test using is_training column ---\n",
        "train_df = df_valid[df_valid['is_training'] == 1]\n",
        "test_df  = df_valid[df_valid['is_training'] == 0]\n",
        "\n",
        "# --- Data Augmentation (Train) ---\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# --- No Augmentation (Test) ---\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "train_gen = train_datagen.flow_from_dataframe(\n",
        "    train_df, x_col='image_path', y_col='bmi',\n",
        "    target_size=(224, 224), class_mode='raw', batch_size=32\n",
        ")\n",
        "\n",
        "test_gen = test_datagen.flow_from_dataframe(\n",
        "    test_df, x_col='image_path', y_col='bmi',\n",
        "    target_size=(224, 224), class_mode='raw', batch_size=32,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# --- VGG19 base + custom regression head ---\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Unfreeze top 8 conv layers\n",
        "for layer in base_model.layers[:-8]:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(1, activation='linear')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "model.compile(optimizer=Adam(1e-4), loss='mean_absolute_error')\n",
        "\n",
        "# --- Callbacks ---\n",
        "early_stop = EarlyStopping(patience=4, restore_best_weights=True)\n",
        "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2,\n",
        "                                min_lr=1e-6, verbose=1)\n",
        "\n",
        "# --- Train the model ---\n",
        "model.fit(train_gen, validation_data=test_gen,\n",
        "          epochs=15,\n",
        "          callbacks=[early_stop, lr_schedule])\n",
        "\n",
        "# --- Evaluate on test set ---\n",
        "y_true = test_df['bmi'].values\n",
        "y_pred = model.predict(test_gen).flatten()\n",
        "genders = test_df['gender'].values\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "r_all, _ = pearsonr(y_true, y_pred)\n",
        "r_male, _ = pearsonr(y_true[genders == 'Male'], y_pred[genders == 'Male'])\n",
        "r_female, _ = pearsonr(y_true[genders == 'Female'], y_pred[genders == 'Female'])\n",
        "\n",
        "print(\"\\nâœ… VGG19 Fine-Tuned (Top 8 Layers + Augmentation + LR Scheduler):\")\n",
        "print(f\"ğŸ“ˆ Pearson r (Overall): {r_all:.3f}\")\n",
        "print(f\"ğŸ‘¦ Pearson r (Male):    {r_male:.3f}\")\n",
        "print(f\"ğŸ‘§ Pearson r (Female):  {r_female:.3f}\")\n",
        "print(f\"ğŸ“ Mean Absolute Error: {mae:.2f}\")\n",
        "\n",
        "# --- Save the model ---\n",
        "model.save('/Users/halleluyamengesha/Desktop/UChicago/_Quarter_3/Machine Learning II/Project/vgg19_ftd_aug_lrs_bmi_model.keras', save_format=\"keras\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "83b2b8da",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Starting TFLite conversion...\n",
            "ğŸ“ Current Working Directory: /Users/halleluyamengesha/Desktop/UChicago/_Quarter_3/Machine Learning II/Project\n",
            "âœ… Found model: vgg19_finetuned_aug_lrsched_bmi_model.keras\n",
            "INFO:tensorflow:Assets written to: /var/folders/qd/9k8sr40n2mq8fbk6jyf7q9tw0000gn/T/tmpw7ribk2_/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/qd/9k8sr40n2mq8fbk6jyf7q9tw0000gn/T/tmpw7ribk2_/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/var/folders/qd/9k8sr40n2mq8fbk6jyf7q9tw0000gn/T/tmpw7ribk2_'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer_4')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  13399268512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13478674272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14501662560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14501658688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14359819360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14359820240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14188864960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14188865136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14188942784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14188942608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14332965456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14332968096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5902515712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5903254272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  6435675296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  5903328880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14189010256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14189007968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14189022720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14189023072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14189020080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14188994224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14188993520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14188994048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14188963968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14188963088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13489272608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13489272432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13489249440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13489250144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13489247152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  13489246624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  6210728880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  6210702848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  6210683824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  6210684880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "âœ… Saved TFLite model to vgg19_finetuned_aug_lrsched_bmi_model_quant.tflite\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1748138178.356070 1017195 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1748138178.356091 1017195 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
          ]
        }
      ],
      "source": [
        "# Convert CNN to .tflite\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"ğŸš€ Starting TFLite conversion...\")\n",
        "\n",
        "# Set working directory (optional if you're already there)\n",
        "os.chdir(\"/Users/halleluyamengesha/Desktop/UChicago/_Quarter_3/Machine Learning II/Project\")\n",
        "print(\"ğŸ“ Current Working Directory:\", os.getcwd())\n",
        "\n",
        "keras_path = \"vgg19_finetuned_aug_lrsched_bmi_model.keras\"\n",
        "assert os.path.exists(keras_path), f\"âŒ Model not found: {keras_path}\"\n",
        "print(\"âœ… Found model:\", keras_path)\n",
        "\n",
        "# --- Load the model (compile=False is safe since you're not retraining) ---\n",
        "model = tf.keras.models.load_model(keras_path, compile=False)\n",
        "\n",
        "# --- Convert to TFLite ---\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# Optional: Post-training quantization to reduce file size and enable edge deployment\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Convert\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save to file\n",
        "tflite_path = \"vgg19_finetuned_aug_lrsched_bmi_model_quant.tflite\"\n",
        "with open(tflite_path, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"âœ… Saved TFLite model to {tflite_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "596ec5e4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Pickled TFLite bytes to vgg19_finetuned_aug_lrsched_bmi_model_quant.pkl\n"
          ]
        }
      ],
      "source": [
        "# Convert to .pkl\n",
        "\n",
        "import pickle\n",
        "\n",
        "pkl_path = \"vgg19_finetuned_aug_lrsched_bmi_model_quant.pkl\"\n",
        "with open(pkl_path, \"wb\") as f:\n",
        "    pickle.dump(tflite_model, f)\n",
        "print(f\"âœ… Pickled TFLite bytes to {pkl_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1397e2ae",
      "metadata": {},
      "source": [
        "### ii) Feature Extraction + Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "651f194d",
      "metadata": {},
      "source": [
        "**Step 1: Extraction - Using Best Fine-Tuned VGG19**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b540e0c6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ Extracting features from 3962 images...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:   3%|â–         | 103/3962 [00:07<04:42, 13.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 100 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:   5%|â–Œ         | 203/3962 [00:15<04:37, 13.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 200 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:   8%|â–Š         | 303/3962 [00:22<04:33, 13.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 300 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  10%|â–ˆ         | 403/3962 [00:30<04:14, 13.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 400 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  13%|â–ˆâ–        | 503/3962 [00:37<04:16, 13.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 500 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  15%|â–ˆâ–Œ        | 603/3962 [00:44<04:06, 13.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 600 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  18%|â–ˆâ–Š        | 703/3962 [00:52<04:04, 13.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 700 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  20%|â–ˆâ–ˆ        | 803/3962 [00:59<04:07, 12.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 800 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  23%|â–ˆâ–ˆâ–       | 903/3962 [01:06<03:43, 13.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 900 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  25%|â–ˆâ–ˆâ–Œ       | 1003/3962 [01:14<03:34, 13.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 1000 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  28%|â–ˆâ–ˆâ–Š       | 1103/3962 [01:21<03:33, 13.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 1100 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  30%|â–ˆâ–ˆâ–ˆ       | 1203/3962 [01:29<03:23, 13.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 1200 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  33%|â–ˆâ–ˆâ–ˆâ–      | 1303/3962 [01:36<03:14, 13.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 1300 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1403/3962 [01:43<03:04, 13.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 1400 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 1503/3962 [01:51<03:02, 13.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 1500 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1603/3962 [01:58<02:55, 13.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 1600 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1703/3962 [02:06<02:47, 13.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 1700 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1803/3962 [02:13<02:36, 13.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 1800 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1903/3962 [02:21<02:29, 13.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 1900 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2003/3962 [02:28<02:25, 13.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 2000 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 2103/3962 [02:36<02:17, 13.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 2100 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 2203/3962 [02:43<02:13, 13.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 2200 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2303/3962 [02:50<02:00, 13.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 2300 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 2403/3962 [02:58<01:54, 13.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 2400 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 2503/3962 [03:05<01:46, 13.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 2500 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 2603/3962 [03:13<01:40, 13.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 2600 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2703/3962 [03:20<01:41, 12.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 2700 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2803/3962 [03:28<01:25, 13.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 2800 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2903/3962 [03:35<01:17, 13.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 2900 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3003/3962 [03:43<01:12, 13.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 3000 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 3103/3962 [03:50<01:02, 13.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 3100 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3203/3962 [03:57<00:54, 13.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 3200 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3303/3962 [04:05<00:47, 13.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 3300 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 3403/3962 [04:12<00:41, 13.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 3400 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 3503/3962 [04:20<00:33, 13.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 3500 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 3603/3962 [04:27<00:27, 13.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 3600 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 3703/3962 [04:35<00:19, 13.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 3700 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 3803/3962 [04:42<00:11, 13.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 3800 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 3903/3962 [04:50<00:04, 13.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Processed 3900 images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3962/3962 [04:54<00:00, 13.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Feature extraction complete.\n",
            "ğŸ”¢ Features shape: (3962, 256)\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input  # works for VGG19 too\n",
        "\n",
        "from tqdm import tqdm  # â† progress bar\n",
        "\n",
        "# --- Load the fine-tuned model ---\n",
        "model = tf.keras.models.load_model(\n",
        "    '/Users/halleluyamengesha/Desktop/UChicago/_Quarter_3/Machine Learning II/Project/vgg19_finetuned_aug_lrsched_bmi_model.keras'\n",
        ")\n",
        "\n",
        "# --- Extract the penultimate layer (Dense 256) as the feature output ---\n",
        "feature_model = Model(inputs=model.input, outputs=model.get_layer(index=-2).output)\n",
        "\n",
        "# --- Preprocessing function ---\n",
        "def preprocess_image(img_path):\n",
        "    img = Image.open(img_path).convert(\"RGB\").resize((224, 224))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return preprocess_input(img_array)\n",
        "\n",
        "# --- Loop through df_valid for feature extraction ---\n",
        "features = []\n",
        "labels = []\n",
        "image_paths = []\n",
        "\n",
        "print(f\"ğŸ”„ Extracting features from {len(df_valid)} images...\\n\")\n",
        "\n",
        "for i, row in tqdm(df_valid.iterrows(), total=len(df_valid), desc=\"Extracting\"):\n",
        "    try:\n",
        "        img_array = preprocess_image(row['image_path'])\n",
        "        feat_vec = feature_model.predict(img_array, verbose=0)[0]  # 256-d\n",
        "        features.append(feat_vec)\n",
        "        labels.append(row['bmi'])\n",
        "        image_paths.append(row['image_path'])\n",
        "\n",
        "        # Optional: log every 100 samples\n",
        "        if i % 100 == 0 and i > 0:\n",
        "            print(f\"âœ… Processed {i} images...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Error on {row['image_path']}: {e}\")\n",
        "\n",
        "# --- Save features and labels ---\n",
        "features_array = np.array(features)\n",
        "labels_array = np.array(labels)\n",
        "\n",
        "# Optionally save to disk\n",
        "np.save(\"vgg19_features.npy\", features_array)\n",
        "np.save(\"vgg19_labels.npy\", labels_array)\n",
        "\n",
        "# Or create a DataFrame if you prefer\n",
        "df_features = pd.DataFrame(features_array)\n",
        "df_features['bmi'] = labels_array\n",
        "df_features['image_path'] = image_paths\n",
        "df_features.to_csv(\"vgg19_features_labeled.csv\", index=False)\n",
        "\n",
        "print(\"\\nâœ… Feature extraction complete.\")\n",
        "print(f\"ğŸ”¢ Features shape: {features_array.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "69b5e83c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Starting feature extractor conversion...\n",
            "INFO:tensorflow:Assets written to: /var/folders/qd/9k8sr40n2mq8fbk6jyf7q9tw0000gn/T/tmp7fimkf5t/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/qd/9k8sr40n2mq8fbk6jyf7q9tw0000gn/T/tmp7fimkf5t/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifact at '/var/folders/qd/9k8sr40n2mq8fbk6jyf7q9tw0000gn/T/tmp7fimkf5t'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer_4')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 256), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  14561128112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14561127056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14561196864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14561196160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14561155904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14561154848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218340128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218339776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218180032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218179328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218182496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218273184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218276176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218275648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218246448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218245920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218201392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218200336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218515728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218515376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218518368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218623280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218624160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218623632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218632352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218631296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218647680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218647152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218585184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218584832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218587824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14218438784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14211831056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  14211830176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "âœ… Saved feature extractor as vgg19_feature_extractor.tflite\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1748138218.074144 1017195 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
            "W0000 00:00:1748138218.074156 1017195 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "print(\"ğŸš€ Starting feature extractor conversion...\")\n",
        "\n",
        "# Load full model\n",
        "full_model = tf.keras.models.load_model(\"vgg19_finetuned_aug_lrsched_bmi_model.keras\")\n",
        "\n",
        "# Extract Dense(256) output layer\n",
        "feature_model = Model(inputs=full_model.input, outputs=full_model.get_layer(index=-2).output)\n",
        "\n",
        "# Convert to TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(feature_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save to disk\n",
        "with open(\"vgg19_feature_extractor.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"âœ… Saved feature extractor as vgg19_feature_extractor.tflite\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6791970",
      "metadata": {},
      "source": [
        "**Step 2: Regression Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3720095",
      "metadata": {},
      "source": [
        "**a) Individual Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2c9ba6bc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 rows with missing is_training\n",
            "Train: 3210\n",
            "Test: 752\n"
          ]
        }
      ],
      "source": [
        "df_features = pd.read_csv(\"vgg19_features_labeled.csv\")\n",
        "\n",
        "# Extract filename from path\n",
        "df_features[\"file\"] = df_features[\"image_path\"].apply(lambda p: os.path.basename(p))\n",
        "df_valid[\"file\"] = df_valid[\"image_path\"].apply(lambda p: os.path.basename(p))\n",
        "\n",
        "# Merge\n",
        "df_features = df_features.merge(df_valid[['file', 'gender', 'is_training']], on='file', how='left')\n",
        "\n",
        "# Check for nulls\n",
        "print(df_features[\"is_training\"].isna().sum(), \"rows with missing is_training\")\n",
        "\n",
        "# Sanity check split counts\n",
        "print(\"Train:\", sum(df_features[\"is_training\"] == 1))\n",
        "print(\"Test:\", sum(df_features[\"is_training\"] == 0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71fca579",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ’¾ Saved Ridge model to models/vgg19_ridge_model.pkl\n",
            "\n",
            "âœ… Ridge\n",
            "ğŸ“ MAE: 5.15\n",
            "ğŸ“ˆ Pearson r (Overall): 0.633\n",
            "ğŸ‘¦ Pearson r (Male):    0.675\n",
            "ğŸ‘§ Pearson r (Female):  0.577\n",
            "ğŸ’¾ Saved Random Forest model to models/vgg19_random_forest_model.pkl\n",
            "\n",
            "âœ… Random Forest\n",
            "ğŸ“ MAE: 4.99\n",
            "ğŸ“ˆ Pearson r (Overall): 0.647\n",
            "ğŸ‘¦ Pearson r (Male):    0.695\n",
            "ğŸ‘§ Pearson r (Female):  0.584\n",
            "ğŸ’¾ Saved SVR model to models/vgg19_svr_model.pkl\n",
            "\n",
            "âœ… SVR\n",
            "ğŸ“ MAE: 5.00\n",
            "ğŸ“ˆ Pearson r (Overall): 0.649\n",
            "ğŸ‘¦ Pearson r (Male):    0.697\n",
            "ğŸ‘§ Pearson r (Female):  0.584\n",
            "ğŸ’¾ Saved KNN model to models/vgg19_knn_model.pkl\n",
            "\n",
            "âœ… KNN\n",
            "ğŸ“ MAE: 5.20\n",
            "ğŸ“ˆ Pearson r (Overall): 0.626\n",
            "ğŸ‘¦ Pearson r (Male):    0.668\n",
            "ğŸ‘§ Pearson r (Female):  0.572\n",
            "ğŸ’¾ Saved MLP model to models/vgg19_mlp_model.pkl\n",
            "\n",
            "âœ… MLP\n",
            "ğŸ“ MAE: 4.98\n",
            "ğŸ“ˆ Pearson r (Overall): 0.649\n",
            "ğŸ‘¦ Pearson r (Male):    0.696\n",
            "ğŸ‘§ Pearson r (Female):  0.586\n",
            "ğŸ’¾ Saved XGB model to models/vgg19_xgb_model.pkl\n",
            "\n",
            "âœ… XGB\n",
            "ğŸ“ MAE: 5.08\n",
            "ğŸ“ˆ Pearson r (Overall): 0.635\n",
            "ğŸ‘¦ Pearson r (Male):    0.675\n",
            "ğŸ‘§ Pearson r (Female):  0.582\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002801 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 65280\n",
            "[LightGBM] [Info] Number of data points in the train set: 3210, number of used features: 256\n",
            "[LightGBM] [Info] Start training from score 32.434370\n",
            "ğŸ’¾ Saved LightGBM model to models/vgg19_lightgbm_model.pkl\n",
            "\n",
            "âœ… LightGBM\n",
            "ğŸ“ MAE: 5.15\n",
            "ğŸ“ˆ Pearson r (Overall): 0.624\n",
            "ğŸ‘¦ Pearson r (Male):    0.676\n",
            "ğŸ‘§ Pearson r (Female):  0.554\n",
            "ğŸ’¾ Saved CatBoost model to models/vgg19_catboost_model.pkl\n",
            "\n",
            "âœ… CatBoost\n",
            "ğŸ“ MAE: 5.05\n",
            "ğŸ“ˆ Pearson r (Overall): 0.645\n",
            "ğŸ‘¦ Pearson r (Male):    0.697\n",
            "ğŸ‘§ Pearson r (Female):  0.575\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "import joblib\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# --- Load features ---\n",
        "df_features = pd.read_csv(\"vgg19_features_labeled.csv\")\n",
        "\n",
        "# --- Merge gender and is_training from df_valid ---\n",
        "df_features[\"file\"] = df_features[\"image_path\"].apply(os.path.basename)\n",
        "df_valid[\"file\"] = df_valid[\"image_path\"].apply(os.path.basename)\n",
        "df_features = df_features.merge(df_valid[[\"file\", \"gender\", \"is_training\"]], on=\"file\", how=\"left\")\n",
        "\n",
        "# --- Check merge worked ---\n",
        "assert df_features[\"is_training\"].notna().all(), \"âŒ Some rows are missing is_training after merge.\"\n",
        "assert df_features[\"gender\"].notna().all(), \"âŒ Some rows are missing gender after merge.\"\n",
        "\n",
        "# --- Prepare features/labels ---\n",
        "X = df_features.drop(columns=[\"bmi\", \"image_path\", \"file\", \"gender\", \"is_training\"]).values\n",
        "y = df_features[\"bmi\"].values\n",
        "genders = df_features[\"gender\"].values\n",
        "split = df_features[\"is_training\"].astype(bool).values  # True = train\n",
        "\n",
        "X_train, X_test = X[split], X[~split]\n",
        "y_train, y_test = y[split], y[~split]\n",
        "g_train, g_test = genders[split], genders[~split]\n",
        "\n",
        "# --- Regressors to compare ---\n",
        "regressors = {\n",
        "    \"Ridge\": Ridge(alpha=1.0),\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"SVR\": SVR(C=10, epsilon=1.0),\n",
        "    \"KNN\": KNeighborsRegressor(n_neighbors=5),\n",
        "    \"MLP\": MLPRegressor(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42),\n",
        "    \"XGB\": xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
        "    \"LightGBM\": lgb.LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
        "    \"CatBoost\": CatBoostRegressor(iterations=100, learning_rate=0.1, verbose=0, random_state=42)\n",
        "}\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# --- Train and evaluate ---\n",
        "for name, model in regressors.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Save the trained model\n",
        "    model_path = f\"models/vgg19_{name.replace(' ', '_').lower()}_model.pkl\"\n",
        "    joblib.dump(model, model_path)\n",
        "    print(f\"ğŸ’¾ Saved {name} model to {model_path}\")\n",
        "\n",
        "    # Evaluate\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r_all, _ = pearsonr(y_test, y_pred)\n",
        "\n",
        "    # Gender-specific Pearson r\n",
        "    try:\n",
        "        r_male, _ = pearsonr(y_test[g_test == \"Male\"], y_pred[g_test == \"Male\"])\n",
        "        r_female, _ = pearsonr(y_test[g_test == \"Female\"], y_pred[g_test == \"Female\"])\n",
        "    except Exception as e:\n",
        "        r_male, r_female = np.nan, np.nan\n",
        "        print(f\"âš ï¸ Gender-specific correlation failed for {name}: {e}\")\n",
        "\n",
        "    print(f\"\\nâœ… {name}\")\n",
        "    print(f\"ğŸ“ MAE: {mae:.2f}\")\n",
        "    print(f\"ğŸ“ˆ Pearson r (Overall): {r_all:.3f}\")\n",
        "    print(f\"ğŸ‘¦ Pearson r (Male):    {r_male:.3f}\")\n",
        "    print(f\"ğŸ‘§ Pearson r (Female):  {r_female:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36de560c",
      "metadata": {},
      "source": [
        "**b) Ensemble Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25798842",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "â³ Training stacking ensemble...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ensemble Training: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "âœ… Ensemble Stacking Regressor\n",
            "ğŸ“ MAE: 4.99\n",
            "ğŸ“ˆ Pearson r (Overall): 0.649\n",
            "ğŸ‘¦ Pearson r (Male):    0.699\n",
            "ğŸ‘§ Pearson r (Female):  0.583\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import StackingRegressor\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Define base learners ---\n",
        "base_learners = [\n",
        "    (\"svr\", SVR(C=10, epsilon=1.0)),\n",
        "    (\"mlp\", MLPRegressor(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)),\n",
        "    (\"rf\", RandomForestRegressor(n_estimators=100, random_state=42)),\n",
        "    (\"catboost\", CatBoostRegressor(iterations=100, learning_rate=0.1, verbose=0, random_state=42))\n",
        "]\n",
        "\n",
        "# --- Meta-learner (Ridge is good for numeric target with small # of features) ---\n",
        "meta_learner = Ridge(alpha=1.0)\n",
        "\n",
        "# --- Build stacking ensemble (let sklearn train everything) ---\n",
        "ensemble = StackingRegressor(\n",
        "    estimators=base_learners,\n",
        "    final_estimator=meta_learner,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# --- Fit ensemble with progress tracking ---\n",
        "print(\"\\nâ³ Training stacking ensemble...\")\n",
        "with tqdm(total=1, desc=\"Ensemble Training\", bar_format=\"{desc}: {bar} {percentage:3.0f}%\") as pbar:\n",
        "    ensemble.fit(X_train, y_train)\n",
        "    pbar.update(1)\n",
        "\n",
        "# --- Predict and evaluate ---\n",
        "y_pred = ensemble.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r_all, _ = pearsonr(y_test, y_pred)\n",
        "r_male, _ = pearsonr(y_test[g_test == \"Male\"], y_pred[g_test == \"Male\"])\n",
        "r_female, _ = pearsonr(y_test[g_test == \"Female\"], y_pred[g_test == \"Female\"])\n",
        "\n",
        "# --- Report ---\n",
        "print(\"\\nâœ… Ensemble Stacking Regressor\")\n",
        "print(f\"ğŸ“ MAE: {mae:.2f}\")\n",
        "print(f\"ğŸ“ˆ Pearson r (Overall): {r_all:.3f}\")\n",
        "print(f\"ğŸ‘¦ Pearson r (Male):    {r_male:.3f}\")\n",
        "print(f\"ğŸ‘§ Pearson r (Female):  {r_female:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
